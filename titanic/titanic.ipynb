{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "- **Goal**\n",
    "You are asked to predict if a passenger survived the sinking of the Titanic or not. \n",
    "For each in the test set, we must predict a 0 or 1 value for the variable.\n",
    "- **Metric**\n",
    "Your score is the percentage of passengers you correctly predict. This is known simply as \"accuracy”.\n",
    "- **Submission File Format**\n",
    "You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n",
    "\n",
    "The file should have exactly 2 columns:\n",
    "\n",
    "PassengerId (sorted in any order)\n",
    "Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n",
    "~~~\n",
    "PassengerId,Survived\n",
    " 892,0\n",
    " 893,1\n",
    " 894,0\n",
    " Etc.\n",
    " ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Load Data\n",
    "First we have to select \"the most\" relevant features to use.\n",
    "After quick inspection of the train.csv file. I select `pclass, sex, age, sibsp, parch, fare, embarked`\n",
    "## 2.1 - Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbify_data(df):\n",
    "    # Convert sex to numeric\n",
    "    df['Sex'].replace('female', 0, inplace=True)\n",
    "    df['Sex'].replace('male', 1, inplace=True)\n",
    "\n",
    "    # Convert embarked to numeric\n",
    "    df['Embarked'].replace('Q', 0, inplace=True)\n",
    "    df['Embarked'].replace('S', 1, inplace=True)\n",
    "    df['Embarked'].replace('C', 2, inplace=True)\n",
    "\n",
    "    # Replace nan age values by the mean of the available ages\n",
    "    age_mean = np.nanmean(df.Age.values)\n",
    "    df['Age'].fillna(age_mean, inplace=True)\n",
    "    \n",
    "    sibsp_mean = np.nanmean(df.SibSp.values)\n",
    "    df['SibSp'].fillna(sibsp_mean, inplace=True)\n",
    "    \n",
    "    parch_mean = np.nanmean(df.Parch.values)\n",
    "    df['Parch'].fillna(parch_mean, inplace=True)\n",
    "    \n",
    "    fare_mean = np.nanmean(df.Fare.values)\n",
    "    df['Fare'].fillna(fare_mean, inplace=True)\n",
    "    \n",
    "    #For missed values\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "def load_train_set():\n",
    "    df = pd.read_csv('train.csv', delimiter = ',')\n",
    "    df = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "    numbify_data(df)\n",
    "    train_set_x = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].values.T\n",
    "    train_set_y = df[['Survived']].values.T\n",
    "    return train_set_x, train_set_y\n",
    "\n",
    "def load_test_set():\n",
    "    df = pd.read_csv('test.csv', delimiter = ',')\n",
    "    df = df[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "    numbify_data(df)\n",
    "    test_set_x = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].values.T\n",
    "    test_set_id = df[['PassengerId']].values.T\n",
    "    return test_set_x, test_set_id\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    return np.abs(x) * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Load training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y = load_train_set()\n",
    "test_set_x, test_set_id = load_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Overview of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 891\n",
      "Number of testing examples: m_test = 418\n"
     ]
    }
   ],
   "source": [
    "m_train = train_set_x.shape[1]\n",
    "m_test = test_set_x.shape[1]\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Simple logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_with_zeros(dim_features):\n",
    "    w = np.zeros((dim_features, 1))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim_features = train_set_x.shape[0]\n",
    "w, b = initialize_with_zeros(dim_features)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Forward and BackWard Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    #forward propagation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = (-1/m) * np.sum(Y*np.log(A) + (1 - Y)*np.log(1-A))\n",
    "    \n",
    "    #backward propagation\n",
    "    dw = (1/m) * np.dot(X, (A-Y).T)\n",
    "    db = (1/m) * np.sum(A-Y)\n",
    "    \n",
    "    grads = {'dw': dw,\n",
    "            'db': db}\n",
    "    return grads, cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        w = w - learning_rate*grads['dw']\n",
    "        b = b - learning_rate*grads['db']\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"cost after iteration\", i,\":\", cost)\n",
    "        costs.append(cost)\n",
    "    params = {'w': w,\n",
    "             'b': b}\n",
    "\n",
    "    grads = {'dw': grads['dw'],\n",
    "             'db': grads['db']}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0 : 0.6931471805599454\n",
      "cost after iteration 1000 : 0.5670028753214277\n",
      "cost after iteration 2000 : 0.5415203581145052\n",
      "cost after iteration 3000 : 0.5238220207746997\n",
      "cost after iteration 4000 : 0.5111000839307397\n",
      "cost after iteration 5000 : 0.5017357245261315\n",
      "cost after iteration 6000 : 0.49469691505506713\n",
      "cost after iteration 7000 : 0.4893020045111173\n",
      "cost after iteration 8000 : 0.485088683688495\n",
      "cost after iteration 9000 : 0.4817369244859513\n",
      "w = [[-0.21752223]\n",
      " [-1.87376673]\n",
      " [-0.00694106]\n",
      " [-0.24935727]\n",
      " [-0.02446017]\n",
      " [ 0.0109378 ]\n",
      " [ 0.45777675]]\n",
      "b = 0.592665735247358\n"
     ]
    }
   ],
   "source": [
    "X = train_set_x\n",
    "Y = train_set_y\n",
    "w, b = initialize_with_zeros(X.shape[0])\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 10000, learning_rate = 0.004, print_cost = True)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0, i] = 1 if A[0, i] >= .5 else 0 \n",
    "        pass\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 38.38383838383839 %\n"
     ]
    }
   ],
   "source": [
    "X_train = train_set_x\n",
    "Y_train = train_set_y\n",
    "\n",
    "np.nan_to_num(X_train, copy=False)\n",
    "np.nan_to_num(Y_train, copy=False)\n",
    "\n",
    "X_test = test_set_x\n",
    "np.nan_to_num(X_test, copy=False)\n",
    "\n",
    "Y_prediction_test = predict(w,b,X_test)\n",
    "Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "np.savetxt('submission.csv', np.hstack([test_set_id.T, Y_prediction_test.T]), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 - Conclusion : Simple logistic regression\n",
    "At this point, we can see that, with a ridiculous train accuracy of 38%, we can safely assume that a simple logistic regression model is far from the best solution to this problem.\n",
    "\n",
    "Let's give it another shot with a more sophisticated model. i.e with more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Neural Network Model\n",
    "\n",
    "We are now going to try with a 1 hidden layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Defining the neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 7\n",
      "The size of the hidden layer is: n_h = 4\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(X_train, Y_train)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Initialize the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    W1 = (np.random.randn(n_h,n_x)*0.01).reshape((n_h, n_x))\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = (np.random.rand(n_y,n_h)*0.01).reshape((n_y, n_h))\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.0078631  -0.00414445  0.00691396 -0.00901321 -0.00595378  0.00380816\n",
      "   0.0017902 ]\n",
      " [-0.00275307  0.00410779 -0.00553137  0.00648442 -0.00814667  0.00735262\n",
      "  -0.0009118 ]\n",
      " [-0.01383613  0.01887677  0.01050753  0.01049961  0.002966    0.01727625\n",
      "   0.01159495]\n",
      " [ 0.00319127 -0.00567171 -0.00089164 -0.00608506 -0.00617312  0.01541913\n",
      "  -0.00217555]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[0.00377988 0.00317726 0.00144991 0.00259223]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1 - A2),1 - Y)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2,A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1 - np.power(A1,2))\n",
    "    dW1 = 1/m * np.dot(dZ1,X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Update Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.005):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 - Integrate all functions into the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, learning_rate = 0.004, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2,Y,parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693251\n",
      "Cost after iteration 10000: 0.494661\n",
      "Cost after iteration 20000: 0.466033\n",
      "Cost after iteration 30000: 0.433574\n",
      "Cost after iteration 40000: 0.430118\n",
      "Cost after iteration 50000: 0.422638\n",
      "Cost after iteration 60000: 0.417044\n",
      "Cost after iteration 70000: 0.411766\n",
      "Cost after iteration 80000: 0.406252\n",
      "Cost after iteration 90000: 0.402037\n",
      "Cost after iteration 100000: 0.399544\n",
      "Cost after iteration 110000: 0.398100\n",
      "Cost after iteration 120000: 0.397185\n",
      "Cost after iteration 130000: 0.396521\n",
      "Cost after iteration 140000: 0.395982\n",
      "Cost after iteration 150000: 0.395504\n",
      "Cost after iteration 160000: 0.395054\n",
      "Cost after iteration 170000: 0.394617\n",
      "Cost after iteration 180000: 0.394197\n",
      "Cost after iteration 190000: 0.393803\n",
      "Cost after iteration 200000: 0.393439\n",
      "Cost after iteration 210000: 0.393105\n",
      "Cost after iteration 220000: 0.392795\n",
      "Cost after iteration 230000: 0.392506\n",
      "Cost after iteration 240000: 0.392235\n",
      "Cost after iteration 250000: 0.391980\n",
      "Cost after iteration 260000: 0.391741\n",
      "Cost after iteration 270000: 0.391524\n",
      "Cost after iteration 280000: 0.391345\n",
      "Cost after iteration 290000: 0.391246\n",
      "Cost after iteration 300000: 0.391364\n",
      "Cost after iteration 310000: 0.391680\n",
      "Cost after iteration 320000: 0.391293\n",
      "Cost after iteration 330000: 0.390613\n",
      "Cost after iteration 340000: 0.390051\n",
      "Cost after iteration 350000: 0.389622\n",
      "Cost after iteration 360000: 0.389285\n",
      "Cost after iteration 370000: 0.389008\n",
      "Cost after iteration 380000: 0.388769\n",
      "Cost after iteration 390000: 0.388557\n",
      "Cost after iteration 400000: 0.388365\n",
      "Cost after iteration 410000: 0.388188\n",
      "Cost after iteration 420000: 0.388023\n",
      "Cost after iteration 430000: 0.387868\n",
      "Cost after iteration 440000: 0.387720\n",
      "Cost after iteration 450000: 0.387580\n",
      "Cost after iteration 460000: 0.387445\n",
      "Cost after iteration 470000: 0.387315\n",
      "Cost after iteration 480000: 0.387189\n",
      "Cost after iteration 490000: 0.387066\n",
      "Cost after iteration 500000: 0.386945\n",
      "Cost after iteration 510000: 0.386827\n",
      "Cost after iteration 520000: 0.386709\n",
      "Cost after iteration 530000: 0.386592\n",
      "Cost after iteration 540000: 0.386475\n",
      "Cost after iteration 550000: 0.386356\n",
      "Cost after iteration 560000: 0.386237\n",
      "Cost after iteration 570000: 0.386116\n",
      "Cost after iteration 580000: 0.385994\n",
      "Cost after iteration 590000: 0.385869\n",
      "Cost after iteration 600000: 0.385742\n",
      "Cost after iteration 610000: 0.385613\n",
      "Cost after iteration 620000: 0.385482\n",
      "Cost after iteration 630000: 0.385348\n",
      "Cost after iteration 640000: 0.385213\n",
      "Cost after iteration 650000: 0.385077\n",
      "Cost after iteration 660000: 0.384939\n",
      "Cost after iteration 670000: 0.384802\n",
      "Cost after iteration 680000: 0.384666\n",
      "Cost after iteration 690000: 0.384531\n",
      "Cost after iteration 700000: 0.384400\n",
      "Cost after iteration 710000: 0.384271\n",
      "Cost after iteration 720000: 0.384146\n",
      "Cost after iteration 730000: 0.384025\n",
      "Cost after iteration 740000: 0.383908\n",
      "Cost after iteration 750000: 0.383795\n",
      "Cost after iteration 760000: 0.383686\n",
      "Cost after iteration 770000: 0.383581\n",
      "Cost after iteration 780000: 0.383480\n",
      "Cost after iteration 790000: 0.383382\n",
      "Cost after iteration 800000: 0.383287\n",
      "Cost after iteration 810000: 0.383196\n",
      "Cost after iteration 820000: 0.383107\n",
      "Cost after iteration 830000: 0.383021\n",
      "Cost after iteration 840000: 0.382938\n",
      "Cost after iteration 850000: 0.382858\n",
      "Cost after iteration 860000: 0.382780\n",
      "Cost after iteration 870000: 0.382704\n",
      "Cost after iteration 880000: 0.382631\n",
      "Cost after iteration 890000: 0.382559\n",
      "Cost after iteration 900000: 0.382490\n",
      "Cost after iteration 910000: 0.382422\n",
      "Cost after iteration 920000: 0.382357\n",
      "Cost after iteration 930000: 0.382293\n",
      "Cost after iteration 940000: 0.382231\n",
      "Cost after iteration 950000: 0.382170\n",
      "Cost after iteration 960000: 0.382111\n",
      "Cost after iteration 970000: 0.382053\n",
      "Cost after iteration 980000: 0.381997\n",
      "Cost after iteration 990000: 0.381942\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X_train, Y_train, 5, num_iterations=1000000, learning_rate=0.003, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2 > .5\n",
    "    predictions = predictions.astype(int)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 83%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         1\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         0\n",
       "19           911         0\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         0\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         0\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         0\n",
       "404         1296         0\n",
       "405         1297         0\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         1\n",
       "409         1301         1\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         1\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train = predict(parameters, X_train)\n",
    "print ('Accuracy on train set: %d' % float((np.dot(Y,predictions_train.T) + np.dot(1-Y,1-predictions_train.T))/float(Y.size)*100) + '%')\n",
    "predictions_test = predict(parameters, X_test)\n",
    "df = pd.DataFrame({'PassengerId':np.squeeze(test_set_id), 'Survived':np.squeeze(predictions_test)})\n",
    "df.to_csv('submission.csv',index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Conclusion - Shallow neural network\n",
    "\n",
    "Now we have an accuracy of `77%` which much better compare to the one we had using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden units: 77.55331088664423 %\n",
      "Accuracy for 2 hidden units: 76.65544332210999 %\n",
      "Accuracy for 3 hidden units: 77.10437710437711 %\n",
      "Accuracy for 4 hidden units: 77.89001122334456 %\n",
      "Accuracy for 5 hidden units: 78.33894500561168 %\n",
      "Accuracy for 7 hidden units: 77.32884399551067 %\n",
      "Accuracy for 14 hidden units: 77.55331088664423 %\n",
      "Accuracy for 20 hidden units: 77.77777777777779 %\n",
      "Accuracy for 50 hidden units: 78.11447811447812 %\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 7, 14, 20, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations = 10000)\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
